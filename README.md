# ABD_Trauma_detection

A deep learning pipeline for multi-organ segmentation and injury classification from abdominal trauma CT scans.

## Features

- The UNet-based segmentation model uses pseudo ground truth labels generated by [TotalSegmentator](https://github.com/wasserth/TotalSegmentator) as a base reference. These were preprocessed and adapted to our training pipeline.
- Configurable via `.ini` files
- Training logs and visualizations included

### Contents
- [Folder Structure](#folder-structure)
- [Requirements](#requirements)
- [Install Packages](#install-packages)
- [Running the Project](#running-the-project)
  - [1. Clone the Repository](#1-clone-the-repository)
  - [2. Training](#2-training)
  - [3. Inference / Evaluation](#3-inference--evaluation)
  - [4. Grad-CAM Visualization](#4-grad-cam-visualization)
- [Notes](#notes)
- [Inference Weights](#inference-weights)
- [Pretrained Weights](#pretrained-weights)
- [Acknowledgements](#acknowledgements)
- [License](#license)

### Folder Structure
```
ABD_Trauma_detection/
‚îú‚îÄ‚îÄ config/                      # Configuration loader and .ini files
‚îÇ   ‚îî‚îÄ‚îÄ config_loader.py         # Load training and testing configurations
‚îÇ   ‚îî‚îÄ‚îÄ multiple/                # Stores .ini files for different class types
‚îÇ       ‚îî‚îÄ‚îÄ pretrain_resnet_unet_multidataset_256_64_nagative.ini  # Sample .ini file (must define model type, path, learning rate, etc.)
‚îú‚îÄ‚îÄ data/                       # Data loader and transforms
‚îú‚îÄ‚îÄ models/                     # Model architectures (UNet, ResNet, etc.)
‚îÇ                               # Used `r3d50_KMS_200ep.pth` as pretrained ResNet50 backbone
‚îú‚îÄ‚îÄ model_engine/               # Training & evaluation pipeline
‚îú‚îÄ‚îÄ utils/                      # Helper functions (loss, metrics, visualization)
‚îú‚îÄ‚îÄ weights/                    # (Ignored in .gitignore) Pretrained model weights
‚îú‚îÄ‚îÄ log/                        # Training logs
‚îú‚îÄ‚îÄ multiorgan_train.py     # Training entry point
‚îú‚îÄ‚îÄ multiorgan_test.py      # Evaluation script
‚îú‚îÄ‚îÄ gradcam.py              # Grad-CAM visualization 
‚îú‚îÄ‚îÄ gradcam_subprocess.py   # Subprocess-based Grad-CAM pipeline
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ requirements.txt

```

> üìÅ `.ini` configuration files should be placed in `config/<class_type>/`, such as `config/multiple/pretrain_resnet_unet_multidataset_256_64_nagative.ini`.

### Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA Version 11.7+
- MONAI 1.3+
- CUDNN 8.4+

### Install Packages

```bash
pip install -r requirements.txt
```

---

### Running the Project
Follow these steps to set up and run the project:

---

#### **1. Clone the Repository**
```bash
git clone https://github.com/yilian26/ABD_Trauma_detection.git
cd ABD_Trauma_detection
```

---

#### **2. Training**

Start training by specifying the configuration file (`.ini`), organ type (e.g., `spleen`, `multiple`), and mode (e.g., `cls` for classification or `seg` for segmentation):

```bash
python3 multiorgan_seg_train.py -f pretrain_resnet_unet_multidataset_256_64_nagative -c multiple -m seg -d multi > log/pretrain_resnet_unet_multidataset_256_64_nagative_seg.log 2>&1
```

- `-f`: Config file name (omit extension, looks in `config/multiple/`)
- `-c`: Class type (`spleen`, `kidney`, `liver`, `multiple`)
- `-m`: Mode (e.g., `cls` for classification)
- `-d`: Dataset source: cgmh, rsna, or multi (default: cgmh)
---

#### **3. Inference / Evaluation**

After training, run the evaluation or testing:

```bash
python3 multiorgan_seg_test.py -f pretrain_resnet_unet_multidataset_256_64_nagative -c multiple -d 0 > log/pretrain_resnet_unet_multidataset_256_64_nagative_test.log 2>&1
```

For RSNA or specific organ testing (e.g., spleen):

```bash
python3 multiorgan_seg_test.py -f pretrain_config_rsna_3label_gaussian_224 -c spleen -d 1 -m cls > log/pretrain_config_rsna_3label_gaussian_224_test.log 2>&1
```

---

#### **4. Grad-CAM Visualization**

To visualize model attention via Grad-CAM or Layer-CAM:

```bash
python3 gradcam.py -f pretrain_resnet_unet_multidataset_256_64_nagative -c multiple > log/pretrain_resnet_unet_multidataset_256_64_nagative_gradcam.log 2>&1
```

---

### Notes

- All config `.ini` files are stored in `config/class_type/`
- Training logs are saved in the `log/` directory
- Pretrained weights are saved in `weights/Multilabel/multiple/<timestamp>/`
- You can freely change `-f`, `-c`, `-m`, and `-d` to fit different use cases

---


### Inference Weights

[Download from Google Drive](https://drive.google.com/drive/folders/1fF4vXQpSHqIaMADiQkCM9W9TDX7n6CKt?usp=drive_link)

Each `.ini` file used for training contains a `[Data output]` section with values like:
```ini
[Data output]
data file name = ['20250422060436']
best accuracy = ['0.8202396754191572']
```
Place the `.pth` files using the following structure:
```
weights/
‚îî‚îÄ‚îÄ Multilabel/
    ‚îî‚îÄ‚îÄ multiple/
        ‚îî‚îÄ‚îÄ 20250422060436/
            ‚îî‚îÄ‚îÄ 0.8202396754191572.pth
```

> Ensure that folder and filename exactly match the `data file name` and `best accuracy` from the `.ini` file.  
> You may use `--select` to specify one if there are multiple; otherwise, it will iterate through all.

---

### Pretrained Weights

- We used `r3d50_KMS_200ep.pth` from:
  [kenshohara/3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch)

## Acknowledgements

We gratefully acknowledge the following repositories:

- [kenshohara/3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch) ‚Äì Used `r3d50_KMS_200ep.pth` as pretrained ResNet50 backbone.
- [yizt/Grad-CAM.pytorch](https://github.com/yizt/Grad-CAM.pytorch/tree/master/detection) ‚Äì Reference implementation for Grad-CAM visualization.


### License

This project is licensed under the [MIT License](LICENSE).

---

