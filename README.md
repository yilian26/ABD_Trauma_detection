# ABD_Trauma_detection

A deep learning pipeline for multi-organ segmentation and injury classification from abdominal trauma CT scans.

## Features

- The UNet-based segmentation model uses pseudo ground truth labels generated by [TotalSegmentator](https://github.com/wasserth/TotalSegmentator) as a base reference. These were preprocessed and adapted to our training pipeline.
- Configurable via `.ini` files
- Training logs and visualizations included


## Contents
- [Folder Structure](#folder-structure)
- [Requirements](#requirements)
- [Install Packages](#install-packages)
- [Running the Project](#running-the-project)
  - [1. Clone the Repository](#1-clone-the-repository)
  - [2. Training](#2-training)
  - [3. Inference / Evaluation](#3-inference--evaluation)
  - [4. Grad-CAM Visualization](#4-grad-cam-visualization)
- [Notes](#notes)
- [Inference Weights](#inference-weights)
- [Pretrained Weights](#pretrained-weights)
- [Acknowledgements](#acknowledgements)
- [License](#license)


## Folder Structure
```
ABD_Trauma_detection/
├── config/                      # Configuration loader and .ini files
│   └── config_loader.py         # Load training and testing configurations
│   └── multiple/                # Stores .ini files for different class types
│       └── pretrain_resnet_unet_multidataset_256_64_nagative.ini  # Sample .ini file (must define model type, path, learning rate, etc.)
├── data/                       # Data loader and transforms
├── models/                     # Model architectures (UNet, ResNet, etc.)
│                               # Used `r3d50_KMS_200ep.pth` as pretrained ResNet50 backbone
├── model_engine/               # Training & evaluation pipeline
├── utils/                      # Helper functions (loss, metrics, visualization)
├── weights/                    # (Ignored in .gitignore) Pretrained model weights
├── log/                        # Training logs
├── multiorgan_train.py     # Training entry point
├── multiorgan_test.py      # Evaluation script
├── gradcam.py              # Grad-CAM visualization 
├── gradcam_subprocess.py   # Subprocess-based Grad-CAM pipeline
├── README.md
├── LICENSE
└── requirements.txt

```

>`.ini` configuration files should be placed in `config/<class_type>/`, such as `config/multiple/pretrain_resnet_unet_multidataset_256_64_nagative.ini`.


## Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA Version 11.7+
- MONAI 1.3+
- CUDNN 8.4+


## Install Packages

```bash
pip install -r requirements.txt
```


## Running the Project
Follow these steps to set up and run the project:


#### **1. Clone the Repository**
```bash
git clone https://github.com/yilian26/ABD_Trauma_detection.git
cd ABD_Trauma_detection
```

---

#### **2. Training**

Start training by specifying the configuration file (`.ini`), organ type (e.g., `spleen`, `multiple`), and mode (e.g., `cls` for classification or `seg` for segmentation):

```bash
python3 multiorgan_seg_train.py -f pretrain_resnet_unet_multidataset_256_64_nagative -c multiple -m seg -d multi > log/pretrain_resnet_unet_multidataset_256_64_nagative_seg.log 2>&1
```

- `-f`: Config file name (omit extension, looks in `config/multiple/`)
- `-c`: Class type (`spleen`, `kidney`, `liver`, `multiple`)
- `-m`: Mode (e.g., `cls` for classification)
- `-d`: Dataset source: cgmh, rsna, or multi (default: cgmh)
---

#### **3. Inference / Evaluation**

After training, run the evaluation or testing:

```bash
python3 multiorgan_seg_test.py -f pretrain_resnet_unet_multidataset_256_64_nagative -c multiple -d 0 > log/pretrain_resnet_unet_multidataset_256_64_nagative_test.log 2>&1
```

For RSNA or specific organ testing (e.g., spleen):

```bash
python3 multiorgan_seg_test.py -f pretrain_config_rsna_3label_gaussian_224 -c spleen -d 1 -m cls > log/pretrain_config_rsna_3label_gaussian_224_test.log 2>&1
```

---

#### **4. Grad-CAM Visualization**

To visualize model attention via Grad-CAM or Layer-CAM:

```bash
python3 gradcam.py -f pretrain_resnet_unet_multidataset_256_64_nagative -c multiple > log/pretrain_resnet_unet_multidataset_256_64_nagative_gradcam.log 2>&1
```


### Notes

- All config `.ini` files are stored in `config/class_type/`
- Training logs are saved in the `log/` directory
- Pretrained weights are saved in `weights/Multilabel/multiple/<timestamp>/`
- You can freely change `-f`, `-c`, `-m`, and `-d` to fit different use cases


## Inference Weights

[Download from Google Drive](https://drive.google.com/drive/folders/1fF4vXQpSHqIaMADiQkCM9W9TDX7n6CKt?usp=drive_link)

Each `.ini` file used for training contains a `[Data output]` section with values like:
```ini
[Data output]
data file name = ['20250422060436']
best accuracy = ['0.8202396754191572']
```
Place the `.pth` files using the following structure:
```
weights/
└── Multilabel/
    └── multiple/
        └── 20250422060436/
            └── 0.8202396754191572.pth
```

> Ensure that folder and filename exactly match the `data file name` and `best accuracy` from the `.ini` file.  
> You may use `--select` to specify one if there are multiple; otherwise, it will iterate through all.


## Pretrained Weights

- We used `r3d50_KMS_200ep.pth` from:
  [kenshohara/3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch)


### References

- Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. [**3D-ResNets-PyTorch**](https://github.com/kenshohara/3D-ResNets-PyTorch) – Pretrained weights `r3d50_KMS_200ep.pth` were used as the ResNet50 backbone in this project.

- Yizt. [**Grad-CAM.pytorch**](https://github.com/yizt/Grad-CAM.pytorch/tree/master/detection) – This project refers to the Grad-CAM implementation for visualizing model attention.


### License

This project is licensed under the [MIT License](LICENSE).

---

